{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvana-ak/uplimit-mlops/blob/main/FINAL_MLOPS_Week_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 3: Monitoring Model Performance and Detecting Data Drift\n",
        "\n",
        "##Introduction to Data Drift\n",
        "In the fast-paced world of e-commerce, change is constant. This week, we'll explore a critical challenge in maintaining machine learning models: data drift. We'll use Lamada's recent expansion into the French market as a real-world example to understand this concept.\n",
        "\n",
        "## Lamada's Expansion to France\n",
        "Exciting news! Lamada has recently expanded its operations to France, opening up a whole new market for their e-commerce platform. While this expansion brings great opportunities, it also introduces new challenges for our sentiment analysis model.\n",
        "### Understanding Data Drift\n",
        "Data drift occurs when the statistical properties of the model's input data change over time, potentially affecting the model's performance. In Lamada's case, the introduction of French language reviews is a perfect example of data drift.\n",
        "### Types of Data Drift\n",
        "\n",
        "1. Data Drift: Changes in the distribution of input features.\n",
        "2. Concept Drift: Changes in the relationship between input features and the target variable.\n",
        "3. Target Drift: Changes in the distribution of the target variable.\n",
        "\n",
        "![Compare ML Models in W&B](https://drive.google.com/uc?id=1D33HeSi85W1Ua5ibQo3wY09KZhAj7oJ8)\n",
        "\n",
        "In our scenario with Lamada, we're primarily dealing with data drift as the language of the reviews (our input feature) has changed.\n",
        "\n",
        "To detect data drift, we'll compare our original dataset (English reviews) with the new data coming in from the French market. We'll use the dataset and model we created and logged in Week 1."
      ],
      "metadata": {
        "id": "ic4V2dLL-fcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Retrieving the Original Dataset and Model\n",
        "First, we need to fetch the following:\n",
        "1. dataset used to train the model\n",
        "2. trained model\n",
        "\n",
        "Which we stored in Week 1 using Weights & Biases (wandb), thankfully we logged them during the train step otherwise we would need to regenerate the data which could potentially lead to issues as there are no guarantees that the data we create is the same one as the data the model was trained on.\n",
        "\n",
        "Don't forget to get your W&B API Key!"
      ],
      "metadata": {
        "id": "3ZggRt99-wD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing all the necessary packages\n",
        "!pip install \\\n",
        "pandas \\\n",
        "scikit-learn \\\n",
        "wandb \\\n",
        "skl2onnx \\\n",
        "onnxruntime \\\n",
        "deep-translator \\\n",
        "evidently"
      ],
      "metadata": {
        "id": "IghDQ0w9PZAV",
        "outputId": "de397b19-a0f2-43dc-d058-f732ca442c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Collecting skl2onnx\n",
            "  Downloading skl2onnx-1.17.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting evidently\n",
            "  Downloading evidently-0.4.38-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Collecting onnx>=1.2.1 (from skl2onnx)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxconverter-common>=1.7.0 (from skl2onnx)\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.10/dist-packages (from evidently) (5.24.1)\n",
            "Requirement already satisfied: statsmodels>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from evidently) (0.14.4)\n",
            "Requirement already satisfied: nltk>=3.6.7 in /usr/local/lib/python3.10/dist-packages (from evidently) (3.8.1)\n",
            "Requirement already satisfied: pydantic>=1.10.13 in /usr/local/lib/python3.10/dist-packages (from evidently) (2.9.2)\n",
            "Collecting litestar>=2.8.3 (from evidently)\n",
            "  Downloading litestar-2.12.1-py3-none-any.whl.metadata (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.9.0 (from evidently)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting uvicorn>=0.22.0 (from uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting watchdog>=3.0.0 (from evidently)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m786.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.3 in /usr/local/lib/python3.10/dist-packages (from evidently) (0.12.5)\n",
            "Requirement already satisfied: rich>=13 in /usr/local/lib/python3.10/dist-packages (from evidently) (13.9.3)\n",
            "Collecting iterative-telemetry>=0.0.5 (from evidently)\n",
            "  Downloading iterative_telemetry-0.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting dynaconf>=3.2.4 (from evidently)\n",
            "  Downloading dynaconf-3.2.6-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.10/dist-packages (from evidently) (2024.8.30)\n",
            "Requirement already satisfied: urllib3>=1.26.19 in /usr/local/lib/python3.10/dist-packages (from evidently) (2.2.3)\n",
            "Requirement already satisfied: fsspec>=2024.6.1 in /usr/local/lib/python3.10/dist-packages (from evidently) (2024.6.1)\n",
            "Collecting ujson>=5.4.0 (from evidently)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting deprecation>=2.1.0 (from evidently)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting uuid6>=2024.7.10 (from evidently)\n",
            "  Downloading uuid6-2024.7.10-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: cryptography>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from evidently) (43.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=43.0.1->evidently) (1.17.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Collecting appdirs (from iterative-telemetry>=0.0.5->evidently)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from iterative-telemetry>=0.0.5->evidently) (3.16.1)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from iterative-telemetry>=0.0.5->evidently) (1.7.0)\n",
            "Requirement already satisfied: anyio>=3 in /usr/local/lib/python3.10/dist-packages (from litestar>=2.8.3->evidently) (3.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from litestar>=2.8.3->evidently) (1.2.2)\n",
            "Collecting httpx>=0.22 (from litestar>=2.8.3->evidently)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting msgspec>=0.18.2 (from litestar>=2.8.3->evidently)\n",
            "  Downloading msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: multidict>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from litestar>=2.8.3->evidently) (6.1.0)\n",
            "Collecting polyfactory>=2.6.3 (from litestar>=2.8.3->evidently)\n",
            "  Downloading polyfactory-2.17.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting rich-click (from litestar>=2.8.3->evidently)\n",
            "  Downloading rich_click-1.8.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.7->evidently) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.7->evidently) (4.66.5)\n",
            "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from pandas[parquet]>=1.3.5->evidently) (16.1.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->evidently) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.13->evidently) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.13->evidently) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13->evidently) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13->evidently) (2.18.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.2->evidently) (0.5.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.3->evidently) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->evidently)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.22.0->uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.22.0->evidently)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3->litestar>=2.8.3->evidently) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=43.0.1->evidently) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.22->litestar>=2.8.3->evidently)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13->evidently) (0.1.2)\n",
            "Collecting faker (from polyfactory>=2.6.3->litestar>=2.8.3->evidently)\n",
            "  Downloading Faker-30.8.1-py3-none-any.whl.metadata (15 kB)\n",
            "Downloading skl2onnx-1.17.0-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evidently-0.4.38-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading dynaconf-3.2.6-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterative_telemetry-0.0.9-py3-none-any.whl (10 kB)\n",
            "Downloading litestar-2.12.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid6-2024.7.10-py3-none-any.whl (6.4 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading polyfactory-2.17.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading rich_click-1.8.3-py3-none-any.whl (35 kB)\n",
            "Downloading Faker-30.8.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, websockets, watchdog, uvloop, uuid6, ujson, python-dotenv, protobuf, mypy-extensions, msgspec, humanfriendly, httptools, h11, dynaconf, deprecation, watchfiles, uvicorn, typing-inspect, onnx, iterative-telemetry, httpcore, faker, deep-translator, coloredlogs, rich-click, polyfactory, onnxruntime, onnxconverter-common, httpx, skl2onnx, litestar, evidently\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 coloredlogs-15.0.1 deep-translator-1.11.4 deprecation-2.1.0 dynaconf-3.2.6 evidently-0.4.38 faker-30.8.1 h11-0.14.0 httpcore-1.0.6 httptools-0.6.4 httpx-0.27.2 humanfriendly-10.0 iterative-telemetry-0.0.9 litestar-2.12.1 msgspec-0.18.6 mypy-extensions-1.0.0 onnx-1.17.0 onnxconverter-common-1.14.0 onnxruntime-1.19.2 polyfactory-2.17.0 protobuf-3.20.2 python-dotenv-1.0.1 rich-click-1.8.3 skl2onnx-1.17.0 typing-inspect-0.9.0 ujson-5.10.0 uuid6-2024.7.10 uvicorn-0.32.0 uvloop-0.21.0 watchdog-5.0.3 watchfiles-0.24.0 websockets-13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "3482bf9ddd1a452f8611abd2cc711569"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "B0a331v5U9VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "run = wandb.init()\n",
        "# Example dataset name: yudhiesh/Drug Review MLOps Uplimit/drug-review-dataset:v2\n",
        "dataset_artifact = run.use_artifact('kbhuvi-uplimit/Drug Review MLOps Uplimit/drug-review-dataset:v1', type='dataset')\n",
        "dataset_artifact_dir = dataset_artifact.download()\n"
      ],
      "metadata": {
        "id": "QkIJRyWVPyFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "dataset_dir = Path(dataset_artifact_dir)\n",
        "train_csv, test_csv, test_probas_csv = dataset_dir / \"train.csv\", dataset_dir / \"test.csv\", dataset_dir / \"test_probas.csv\"\n",
        "train_df, test_df, test_probas = pd.read_csv(train_csv), pd.read_csv(test_csv), pd.read_csv(test_probas_csv)"
      ],
      "metadata": {
        "id": "ovEF3SK3PYQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "nnTtO4HhV3qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "Nt0YzQKIV5wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_probas.head()"
      ],
      "metadata": {
        "id": "Ae3xrBNUV7au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class SentimentLabel(str, Enum):\n",
        "    NEGATIVE = \"NEGATIVE\"\n",
        "    NEUTRAL = \"NEUTRAL\"\n",
        "    POSITIVE = \"POSITIVE\"\n",
        "\n",
        "\n",
        "LABEL_CLASS_TO_NAME = {\n",
        "    0: SentimentLabel.NEGATIVE.value,\n",
        "    1: SentimentLabel.NEUTRAL.value,\n",
        "    2: SentimentLabel.POSITIVE.value,\n",
        "}"
      ],
      "metadata": {
        "id": "TUpcV2LbpNmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "test_df['prob_NEGATIVE'] = test_probas['Negative']\n",
        "test_df['prob_NEUTRAL'] = test_probas['Neutral']\n",
        "test_df['prob_POSITIVE'] = test_probas['Positive']\n",
        "\n",
        "test_df['predicted_label'] = test_probas.idxmax(axis=1).map({'Negative': 0, 'Neutral': 1, 'Positive': 2})\n",
        "test_df['predicted_sentiment'] = test_df['predicted_label'].map(LABEL_CLASS_TO_NAME)\n",
        "column_order = ['text', 'label', 'prob_NEGATIVE', 'prob_NEUTRAL', 'prob_POSITIVE', 'predicted_label', 'predicted_sentiment']\n",
        "test_df = test_df[column_order]"
      ],
      "metadata": {
        "id": "hl-DTcvErhRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "yl7yHDZ3rxz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Simulating Data Drift\n",
        "In a real-world scenario, data drift would occur naturally over time. For our learning purposes, we'll simulate this drift by translating a sample of our English reviews to French.\n",
        "\n",
        "### [OPTIONAL] Add in your own kind of pertubations to the reviews\n",
        "You can try the following:\n",
        "1. **Spelling Errors**: Introduce random spelling mistakes to simulate typos in reviews.\n",
        "2. **Emoji Usage**: Add emojis to reviews to simulate changing trends in online communication.\n",
        "3. **Text Shortening**: Simulate the trend of shorter, more concise reviews.\n"
      ],
      "metadata": {
        "id": "l54dpjaQ-2Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "def translate_str(review: str) -> str:\n",
        "    translator = GoogleTranslator(source='auto', target='fr')\n",
        "    return translator.translate(review)"
      ],
      "metadata": {
        "id": "mI5ZUZ0gYFpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "SAMPLE_SIZE = 100\n",
        "\n",
        "data_drift_df = train_df.sample(SAMPLE_SIZE, random_state=SEED)"
      ],
      "metadata": {
        "id": "FLFMaYoTkgrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_drift_df.head()"
      ],
      "metadata": {
        "id": "Z48IjQ8YknZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will take a couple of minutes!\n",
        "\n",
        "data_drift_df['text'] = data_drift_df['text'].apply(lambda row: translate_str(row))"
      ],
      "metadata": {
        "id": "SbEqUl0Qk2MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_drift_df.head()"
      ],
      "metadata": {
        "id": "i4YZmQ7YlECF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Making Predictions on the New Data\n",
        "Now that we have our \"French\" reviews, let's use our original model to make predictions on this new data."
      ],
      "metadata": {
        "id": "z88YKt9R_AKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "\n",
        "\n",
        "run = wandb.init()\n",
        "# Example name of model: yudhiesh/model-registry/Drugs Review MLOps Uplimit:v1\n",
        "downloaded_model_path = run.use_model(\n",
        "    name=\"kbhuvi-uplimit/Drug Review MLOps Uplimit/run-y3m59we9-logreg_model_LR_train_size_1000.onnx:v0\"\n",
        ")\n",
        "\n",
        "sess = rt.InferenceSession(downloaded_model_path, providers=[\"CPUExecutionProvider\"])\n",
        "input_name = sess.get_inputs()[0].name\n",
        "query = \"I loved the product!\"\n",
        "_, probas = sess.run(None, {input_name: np.array([[query]])})\n",
        "print(probas[0])"
      ],
      "metadata": {
        "id": "MbBaj0BGlcSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_batch(sess, texts):\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    # Convert list of texts to a 2D numpy array\n",
        "    input_data = np.array([[text] for text in texts])\n",
        "    _, probas = sess.run(None, {input_name: input_data})\n",
        "    return probas\n",
        "\n",
        "probas = predict_batch(sess, data_drift_df['text'].values)"
      ],
      "metadata": {
        "id": "aeQc5CaSoRwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probas_array = np.array([[prob[0], prob[1], prob[2]] for prob in probas])\n",
        "\n",
        "print(\"Shape of probas_array:\", probas_array.shape)\n",
        "print(\"First few rows of probas_array:\", probas_array[:5])\n",
        "\n",
        "for i, label in LABEL_CLASS_TO_NAME.items():\n",
        "    data_drift_df[f'prob_{label}'] = probas_array[:, i]\n",
        "\n",
        "\n",
        "data_drift_df['predicted_label'] = np.argmax(probas_array, axis=1)\n",
        "data_drift_df['predicted_sentiment'] = data_drift_df['predicted_label'].map(LABEL_CLASS_TO_NAME)"
      ],
      "metadata": {
        "id": "eBgJ2pdRpRzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_drift_df.head()"
      ],
      "metadata": {
        "id": "ZE0BjIaBqGS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Analyzing Data Drift\n",
        "To analyze the data drift, we'll use the [Evidently](https://github.com/evidentlyai/evidently/tree/main) library, which provides tools for monitoring machine learning models in production.\n",
        "\n",
        "The performance report will show how our model's performance has changed when applied to the French reviews. We expect to see a significant drop in performance metrics like accuracy and F1-score.\n",
        "The data drift report will highlight changes in the statistical properties of our text data. We should observe significant drift in features like average word length, unique word count, and character distributions due to the change in language."
      ],
      "metadata": {
        "id": "z91X81Q3_E8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evidently.pipeline.column_mapping import ColumnMapping\n",
        "\n",
        "from evidently.report import Report\n",
        "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n",
        "from evidently.metric_preset import ClassificationPreset\n",
        "from evidently.metrics import ClassificationQualityMetric, TextDescriptorsDriftMetric, ColumnDriftMetric\n",
        "\n",
        "column_mapping = ColumnMapping()\n",
        "\n",
        "column_mapping.target = 'label'\n",
        "column_mapping.prediction = 'predicted_label'\n",
        "column_mapping.text_features = ['text']\n",
        "column_mapping.numerical_features = []\n",
        "column_mapping.task = 'classification'\n",
        "column_mapping.categorical_features = []"
      ],
      "metadata": {
        "id": "scw85QVesdyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_report = Report(metrics=[\n",
        "    ClassificationQualityMetric()\n",
        "])\n",
        "\n",
        "performance_report.run(reference_data=test_df, current_data=data_drift_df,\n",
        "                        column_mapping=column_mapping)"
      ],
      "metadata": {
        "id": "-p-jtJxzqcJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_report.show()"
      ],
      "metadata": {
        "id": "qlNI5fm3zRF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_drift_dataset_report = Report(metrics=[\n",
        "    ColumnDriftMetric(column_name='text')\n",
        "])\n",
        "\n",
        "data_drift_dataset_report.run(reference_data=test_df,\n",
        "                              current_data=data_drift_df,\n",
        "                              column_mapping=column_mapping)\n",
        "data_drift_dataset_report.show()"
      ],
      "metadata": {
        "id": "2N59WE3xzZyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_drift_report = Report(\n",
        "    metrics=[\n",
        "        TextDescriptorsDriftMetric(column_name='text'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_drift_report.run(reference_data=test_df, current_data=data_drift_df, column_mapping=column_mapping)\n",
        "data_drift_report.show()"
      ],
      "metadata": {
        "id": "pnRBAlMq00k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Analyze the Performance Report Results\n",
        "\n",
        "After running the performance report, you should see a comparison between the current (French) and reference (English) datasets. Your task is to analyze these results and explain their significance.\n",
        "\n",
        "1. Examine each metric (Accuracy, Precision, Recall, F1) and describe how it has changed from the reference to the current data.\n",
        "\n",
        "2. Explain why you think each metric has changed in the way it has. Consider the nature of the data drift we've introduced (English to French translation).\n",
        "\n",
        "3. Discuss what these changes mean for Lamada's sentiment analysis system as they expand into the French market. What are the potential business implications?\n",
        "\n",
        "4. Propose strategies Lamada could consider to address this performance degradation.\n",
        "\n",
        "5. Reflect on why this example demonstrates the importance of continuous monitoring in ML systems, especially for businesses operating in diverse markets.\n",
        "\n",
        "Your analysis should be comprehensive, touching on all the points above. Use the specific numbers from the performance report to support your explanations. Remember to consider both the technical aspects of the model's performance and the real-world business implications for Lamada.\n",
        "\n",
        "**Hint**: Pay special attention to metrics that have changed dramatically. Think about what each metric represents and how the language change might affect the model's ability to make correct predictions."
      ],
      "metadata": {
        "id": "NIWeNu0d_kJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR ANSWER GOES HERE"
      ],
      "metadata": {
        "id": "tqnBLkmXAxcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## [OPTIONAL] TODO: Retrain the Model on French Data\n",
        "\n",
        "Now that we've identified the performance degradation due to data drift, let's attempt to address it by retraining our model on the new French data. This exercise will help you understand how model retraining can mitigate the effects of data drift. Follow how we trained and evaluted the model in Week 1!\n",
        "\n",
        "**NOTE**: As we started off using `LogisticRegression` from scikit-learn we will have to perform a stateless retraining, where we retrain from scratch, as the current implementation does not support incremental training as per the scikit-learn documentation [here](https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning).\n",
        "\n",
        "---\n",
        "## [OPTIONAL] TODO: Implement Canary Deployment with Ray Serve\n",
        "In this advanced exercise, you'll implement a canary deployment strategy for your sentiment analysis model using Ray Serve. This approach allows you to gradually roll out a new version of your model while still serving the old version, reducing risk and allowing for easy rollback if issues arise. In a production deployment we would have to incrementally rollout this new model taking into account model/business metrics.\n",
        "\n",
        "```python\n",
        "@serve.deployment()\n",
        "class Canary:\n",
        "    def __init__(self, old_model: DeploymentHandle, new_model: DeploymentHandle, canary_percent: float):\n",
        "        self.old_model = old_model\n",
        "        self.new_model = new_model\n",
        "        self.canary_percent = canary_percent\n",
        "\n",
        "    async def predict(self, request: SimpleModelRequest) -> SimpleModelResponse:\n",
        "        if random.random() > self.canary_percent:\n",
        "            results = await self.old_model.predict.remote(request.review)\n",
        "        else:\n",
        "            results = await self.new_model.predict.remote(request.review)\n",
        "        return SimpleModelResponse.model_validate(results.model_dump())\n",
        "\n",
        "@serve.deployment()\n",
        "@serve.ingress(app)\n",
        "class APIIngress:\n",
        "    def __init__(self, canary_handle: DeploymentHandle) -> None:\n",
        "        self.handle = canary_handle\n",
        "\n",
        "    @app.post(\"/predict\")\n",
        "    async def predict(self, request: SimpleModelRequest):\n",
        "        return await self.handle.predict.remote(request)\n",
        "\n",
        "```\n",
        "\n",
        "### Testing the Canary Deployment:\n",
        "After deploying your canary setup, test it to ensure it's working correctly:\n",
        "\n",
        "1. Send multiple requests to the `/predict` endpoint.\n",
        "2. Log the responses to see which model version is being used for each request.\n",
        "3. Verify that approximately 20% of requests are being routed to the new model.\n",
        "---"
      ],
      "metadata": {
        "id": "ntocG3F4AZ6H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHqTWzhFCaZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}